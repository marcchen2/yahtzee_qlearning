Episode 100: Training score = 38.0, Epsilon = 0.998
Episode 200: Training score = 24.0, Epsilon = 0.996
Episode 300: Training score = 32.0, Epsilon = 0.994
Episode 400: Training score = 32.0, Epsilon = 0.992
Episode 500: Training score = 52.0, Epsilon = 0.990
Episode 600: Training score = 16.0, Epsilon = 0.988
Episode 700: Training score = 88.0, Epsilon = 0.986
Episode 800: Training score = 16.0, Epsilon = 0.984
Episode 900: Training score = 26.0, Epsilon = 0.981
Evaluation after episode 1000: Average score over 100 games = 55.3
Episode 1000: Training score = 24.0, Epsilon = 0.979
Episode 1100: Training score = 12.0, Epsilon = 0.977
Episode 1200: Training score = 36.0, Epsilon = 0.975
Episode 1300: Training score = 44.0, Epsilon = 0.973
Episode 1400: Training score = 64.0, Epsilon = 0.971
Traceback (most recent call last):
  File "/home/mc5635/yahtzee/yahtzee_rl/train_gpu.py", line 316, in <module>
    dqn, target_dqn = train_dqn(make_env,
  File "/home/mc5635/yahtzee/yahtzee_rl/train_gpu.py", line 269, in train_dqn
    s_batch, a_batch, r_batch, s2_batch, d_batch = replay_buffer.sample(batch_size)
  File "/home/mc5635/yahtzee/yahtzee_rl/dqn_agent.py", line 50, in sample
    torch.tensor(dones, dtype=torch.float32).to(self.device)
KeyboardInterrupt
