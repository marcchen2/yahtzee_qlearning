
# To Run Demo:

`pip install -r requirements`

`python3 main_ui.py`

Note: If I run the performance mode over 1000 games, I get ~210 median, ~211 mean. 



# Key Resources:

Used ChatGPT and/or DeepSeek to: 
- Create the baseline code for the Yahtzee environment. 
- Help me understand and create snippets of code for specific training loop techniques (i.e. Prioritized Experience Replay)
- Debug the training pipeline.
- Make suggestions on NN architecture and Q Learning techniques.
- Help me write the Gradio UI elements. 


Weights and Biases to track performance of Models


And these informational resources:

Wang, Z., Schaul, T., Hessel, M., van Hasselt, H., Lanctot, M., & de Freitas, N. (2016). Dueling Network Architectures for Deep Reinforcement Learning. arXiv preprint arXiv:1511.06581. Available at https://arxiv.org/pdf/1511.06581


Kang, M., & Schroeder, L. (2018). Reinforcement Learning for Solving Yahtzee. Stanford University. Available at https://web.stanford.edu/class/aa228/reports/2018/final75.pdf


Hessel, M., Modayil, J., van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M., & Silver, D. (2017). Rainbow: Combining Improvements in Deep Reinforcement Learning. arXiv preprint arXiv:1710.02298. Available at https://arxiv.org/pdf/1710.02298


Brunton, S. (2022, January 15). Q-Learning: Model Free Reinforcement Learning and Temporal Difference Learning [Video]. YouTube. https://www.youtube.com/watch?v=0iqz4tcKN58


Pytorch Documentation

